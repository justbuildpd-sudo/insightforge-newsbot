#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
êµ­íšŒì˜ì› ë‰´ìŠ¤ LDA ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
- ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°ì— ëŒ€í•´ LDA í† í”½ ëª¨ë¸ë§ ìˆ˜í–‰
- ì´ìŠˆë³„ë¡œ ë¶„ë¥˜í•˜ì—¬ ì €ì¥
"""

import json
import os
from collections import Counter
from datetime import datetime

# LDA ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì—†ìœ¼ë©´ ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ ì‚¬ìš©)
try:
    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
    from sklearn.decomposition import LatentDirichletAllocation
    USE_SKLEARN = True
except ImportError:
    USE_SKLEARN = False
    print("âš ï¸  scikit-learnì´ ì—†ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")


INPUT_FILE = "news_data/assembly_member_news.json"
OUTPUT_FILE = "InsightForge/Resources/assembly_member_lda_analysis.json"


def extract_nouns_simple(text):
    """ê°„ë‹¨í•œ ëª…ì‚¬ ì¶”ì¶œ (ë¶ˆìš©ì–´ ì œê±°)"""
    stopwords = set([
        "ì´", "ê°€", "ì„", "ë¥¼", "ì˜", "ì—", "ì—ì„œ", "ìœ¼ë¡œ", "ë¡œ", "ì€", "ëŠ”", "ê³¼", "ì™€", "ë„", "ë§Œ",
        "í•˜ë‹¤", "ë˜ë‹¤", "ì´ë‹¤", "ìˆë‹¤", "ì—†ë‹¤", "í•œë‹¤", "ëœë‹¤", "í–ˆë‹¤", "ëë‹¤", "í•˜ëŠ”", "ë˜ëŠ”",
        "ë“±", "ë°", "ìœ„í•´", "í†µí•´", "ëŒ€í•œ", "ìœ„í•œ", "ê´€ë ¨", "ë”°ë¥¸", "ê²ƒ", "ìˆ˜", "ì¤‘", "ë•Œ"
    ])
    
    verb_endings = ["í•˜ë‹¤", "ë˜ë‹¤", "ì´ë‹¤", "ìˆë‹¤", "ì—†ë‹¤", "í–ˆë‹¤", "ëë‹¤", "í•œë‹¤", "ëœë‹¤", "í•˜ëŠ”", "ë˜ëŠ”", "ìˆëŠ”"]
    
    words = text.split()
    nouns = []
    
    for word in words:
        cleaned = word.strip('.,!?()[]"\':;')
        
        if len(cleaned) >= 2 and cleaned not in stopwords:
            # ë™ì‚¬/í˜•ìš©ì‚¬ ì–´ë¯¸ ì²´í¬
            is_verb = any(cleaned.endswith(ending) for ending in verb_endings)
            if not is_verb and not cleaned.isdigit():
                nouns.append(cleaned)
    
    return nouns


def classify_articles_by_keywords(articles):
    """í‚¤ì›Œë“œ ê¸°ë°˜ ê¸°ì‚¬ ë¶„ë¥˜ (ì„¸ë¶€ í† í”½)"""
    
    # ì´ìŠˆ ì¹´í…Œê³ ë¦¬ ì •ì˜ (ì„¸ë¶€ ë¶„ë¥˜)
    categories = {
        "êµ­ì •ê°ì‚¬Â·ì§ˆì˜": ["êµ­ì •ê°ì‚¬", "ì§ˆì˜", "ë‹µë³€", "ì§€ì ", "ì§ˆë¬¸", "ì²­ë¬¸íšŒ"],
        "ì˜ˆì‚°Â·ì¬ì •": ["ì˜ˆì‚°", "ì˜ˆì‚°ì•ˆ", "ì¬ì •", "ì„¸ê¸ˆ", "ì„¸ì¶œ", "ì„¸ì…", "ê²°ì‚°"],
        "ë²•ì•ˆÂ·ì…ë²•": ["ë²•ì•ˆ", "ë²•ë¥ ", "ê°œì •", "ì…ë²•", "ì¡°ë¡€", "ì‹¬ì˜", "ì˜ê²°", "ë°œì˜"],
        "ì§€ì—­ê°œë°œ": ["ê°œë°œ", "ì¬ê°œë°œ", "ì¬ê±´ì¶•", "ì‹ ì¶•", "ê±´ì„¤", "ì¡°ì„±"],
        "êµí†µÂ·ì¸í”„ë¼": ["êµí†µ", "ë„ë¡œ", "ì§€í•˜ì² ", "ë²„ìŠ¤", "ì£¼ì°¨", "ì² ë„"],
        "ì£¼íƒÂ·ë¶€ë™ì‚°": ["ì£¼íƒ", "ì•„íŒŒíŠ¸", "ë¶€ë™ì‚°", "ì„ëŒ€", "ë¶„ì–‘"],
        "ë³µì§€Â·ì˜ë£Œ": ["ë³µì§€", "ì˜ë£Œ", "ê±´ê°•", "ë³‘ì›", "ë³´ê±´", "ëŒë´„"],
        "ì¼ìë¦¬Â·ê²½ì œ": ["ì¼ìë¦¬", "ê³ ìš©", "ì²­ë…„", "ì·¨ì—…", "ê²½ì œ", "ì†Œìƒê³µì¸"],
        "êµìœ¡Â·ë³´ìœ¡": ["êµìœ¡", "í•™êµ", "í•™ìƒ", "ì–´ë¦°ì´ì§‘", "ìœ ì¹˜ì›", "ë³´ìœ¡"],
        "ë¬¸í™”Â·ì²´ìœ¡": ["ë¬¸í™”", "ì²´ìœ¡", "ì˜ˆìˆ ", "ì¶•ì œ", "ê³µì—°", "ë„ì„œê´€"],
        "í™˜ê²½Â·ê¸°í›„": ["í™˜ê²½", "ê¸°í›„", "ì“°ë ˆê¸°", "ì¬í™œìš©", "ëŒ€ê¸°", "ë…¹ì§€"],
        "ì•ˆì „Â·ì¬ë‚œ": ["ì•ˆì „", "ì¬ë‚œ", "ë°©ì—­", "ì†Œë°©", "ë²”ì£„", "ì¹˜ì•ˆ"],
        "ì •ì±…ë°œí‘œ": ["ì •ì±…", "ê³µì•½", "ë°œí‘œ", "ê³„íš", "ë°©ì•ˆ", "ì¶”ì§„"],
        "ë¯¼ì›Â·ì£¼ë¯¼": ["ë¯¼ì›", "ì£¼ë¯¼", "ì²­ì›", "ê°„ë‹´íšŒ", "ì„¤ëª…íšŒ", "í† ë¡ íšŒ"],
        "ê¸°íƒ€": []
    }
    
    classified = {cat: [] for cat in categories.keys()}
    categorized_links = set()
    
    # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜
    for category, keywords in categories.items():
        if category == "ê¸°íƒ€":
            continue
            
        for article in articles:
            if article['link'] in categorized_links:
                continue
                
            text = article['title'] + " " + article.get('description', '')
            
            # í‚¤ì›Œë“œ ë§¤ì¹­
            if any(keyword in text for keyword in keywords):
                classified[category].append(article)
                categorized_links.add(article['link'])
    
    # ë¶„ë¥˜ë˜ì§€ ì•Šì€ ê¸°ì‚¬ëŠ” "ê¸°íƒ€"ë¡œ
    for article in articles:
        if article['link'] not in categorized_links:
            classified["ê¸°íƒ€"].append(article)
    
    return classified


def extract_top_keywords_from_articles(articles, top_n=10):
    """ê¸°ì‚¬ì—ì„œ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    all_nouns = []
    
    for article in articles:
        text = article['title'] + " " + article.get('description', '')
        nouns = extract_nouns_simple(text)
        all_nouns.extend(nouns)
    
    # ë¹ˆë„ ê³„ì‚°
    word_freq = Counter(all_nouns)
    top_keywords = word_freq.most_common(top_n)
    
    return [(word, count) for word, count in top_keywords]


def analyze_member_news(member_name, news_data):
    """íŠ¹ì • êµ­íšŒì˜ì›ì˜ ë‰´ìŠ¤ ë¶„ì„"""
    articles = news_data.get('news', [])
    
    if not articles:
        return None
    
    print(f"\nğŸ“Š ë¶„ì„ ì¤‘: {member_name} ({len(articles)}ê±´)")
    
    # ì´ìŠˆë³„ ë¶„ë¥˜
    classified = classify_articles_by_keywords(articles)
    
    # ê° ì´ìŠˆë³„ í‚¤ì›Œë“œ ì¶”ì¶œ
    issues = []
    for category, cat_articles in classified.items():
        if not cat_articles:
            continue
        
        top_keywords = extract_top_keywords_from_articles(cat_articles, top_n=10)
        
        # ê¸°ì‚¬ë¥¼ ë‚ ì§œë³„ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        sorted_articles = sorted(cat_articles, key=lambda x: x.get('pubDate', ''), reverse=True)
        
        issue = {
            'category': category,
            'count': len(cat_articles),
            'top_keywords': top_keywords,
            'articles': sorted_articles[:10]  # ìµœì‹  10ê±´ ì €ì¥
        }
        issues.append(issue)
        
        print(f"  âœ… {category}: {len(cat_articles)}ê±´")
    
    return {
        'member_info': news_data['member_info'],
        'total_count': len(articles),
        'last_updated': news_data.get('last_updated', ''),
        'collected_date': news_data.get('collected_date', ''),
        'issues': issues
    }


def main():
    """ë©”ì¸ ë¶„ì„ í”„ë¡œì„¸ìŠ¤"""
    print("\n" + "="*60)
    print("ğŸ“Š êµ­íšŒì˜ì› ë‰´ìŠ¤ LDA ë¶„ì„ ì‹œì‘")
    print("="*60 + "\n")
    
    # ìˆ˜ì§‘ëœ ë°ì´í„° ë¡œë“œ
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {INPUT_FILE}")
        print("ğŸ’¡ ë¨¼ì € collect_assembly_member_news.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)
    
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(raw_data)}ëª…")
    
    # ê° êµ­íšŒì˜ì›ë³„ ë¶„ì„
    analyzed_data = {}
    
    for member_name, news_data in raw_data.items():
        result = analyze_member_news(member_name, news_data)
        if result:
            analyzed_data[member_name] = result
    
    # ê²°ê³¼ ì €ì¥
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(analyzed_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ë¶„ì„ ì™„ë£Œ: {OUTPUT_FILE}")
    print(f"ğŸ“Š ì´ {len(analyzed_data)}ëª…ì˜ êµ­íšŒì˜ì› ë°ì´í„° ìƒì„±")
    
    # í†µê³„ ì¶œë ¥
    total_articles = sum(d['total_count'] for d in analyzed_data.values())
    total_issues = sum(len(d['issues']) for d in analyzed_data.values())
    
    print(f"ğŸ“° ì „ì²´ ê¸°ì‚¬: {total_articles}ê±´")
    print(f"ğŸ·ï¸  ì „ì²´ ì´ìŠˆ: {total_issues}ê°œ")
    
    print("\n" + "="*60)


if __name__ == "__main__":
    main()

