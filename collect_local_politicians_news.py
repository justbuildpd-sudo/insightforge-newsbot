#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì§€ë°© ì •ì¹˜ì¸ í–‰ì •ì‚¬ë¬´ê°ì‚¬ ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
- ì‹œì¥, ì‹œì˜ì›, êµ¬ì²­ì¥, êµ¬ì˜ì› ëŒ€ìƒ
- ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API ì‚¬ìš©
"""

import requests
import json
import time
from datetime import datetime
import os
import schedule

# ë„¤ì´ë²„ API ì¸ì¦ ì •ë³´ (ì§€ë°© ì •ì¹˜ì¸ìš©)
CLIENT_ID = "ULDLTGiPvrrPBgbuydSm"
CLIENT_SECRET = "uO5mu7UQBg"

# API ì—”ë“œí¬ì¸íŠ¸
NAVER_NEWS_API_URL = "https://openapi.naver.com/v1/search/news.json"

# ë°ì´í„° ì €ì¥ ê²½ë¡œ
OUTPUT_DIR = "news_data"
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "local_politicians_news.json")

# ë°ì´í„° íŒŒì¼ ê²½ë¡œ
MAYOR_FILE = "seoul_mayor_8th_real.json"
SI_UIWON_FILE = "seoul_si_uiwon_8th_real.json"
GU_MAYOR_FILE = "seoul_gu_mayor_8th.json"
GU_UIWON_FILE = "seoul_gu_uiwon_8th_real.json"


def check_internet_connection():
    """ì¸í„°ë„· ì—°ê²° í™•ì¸"""
    try:
        response = requests.get("https://www.google.com", timeout=5)
        return True
    except:
        return False


def search_naver_news(query, display=10, start=1, retry_count=3):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
    headers = {
        "X-Naver-Client-Id": CLIENT_ID,
        "X-Naver-Client-Secret": CLIENT_SECRET
    }
    
    params = {
        "query": query,
        "display": display,
        "start": start,
        "sort": "date"  # ìµœì‹ ìˆœ ì •ë ¬
    }
    
    for attempt in range(retry_count):
        try:
            response = requests.get(NAVER_NEWS_API_URL, headers=headers, params=params, timeout=10)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.Timeout:
            print(f"  âš ï¸  íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt + 1}/{retry_count})")
            if attempt < retry_count - 1:
                time.sleep(2)
        except requests.exceptions.ConnectionError:
            print(f"  âš ï¸  ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{retry_count})")
            if attempt < retry_count - 1:
                time.sleep(5)
        except requests.exceptions.HTTPError as e:
            print(f"  âŒ HTTP ì˜¤ë¥˜: {e.response.status_code}")
            if e.response.status_code == 429:
                print(f"  â³ API í˜¸ì¶œ í•œë„ ì´ˆê³¼, 60ì´ˆ ëŒ€ê¸°...")
                time.sleep(60)
            break
        except requests.exceptions.RequestException as e:
            print(f"  âŒ API ìš”ì²­ ì‹¤íŒ¨: {e}")
            break
    
    return None


def load_mayor_data():
    """ì‹œì¥ ë°ì´í„° ë¡œë“œ"""
    # ì„œìš¸ì‹œì¥ì€ ê³ ì • (ì˜¤ì„¸í›ˆ)
    return [{
        'name': 'ì˜¤ì„¸í›ˆ',
        'position': 'ì‹œì¥',
        'district': 'ì„œìš¸íŠ¹ë³„ì‹œ',
        'party': 'êµ­ë¯¼ì˜í˜'
    }]


def load_si_uiwon_data():
    """ì‹œì˜ì› ë°ì´í„° ë¡œë“œ"""
    if not os.path.exists(SI_UIWON_FILE):
        return []
    
    with open(SI_UIWON_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    politicians = []
    for district_data in data.values():
        if isinstance(district_data, list):
            for member in district_data:
                if isinstance(member, dict):
                    name = member.get('name', '').split('\n')[0]
                    if name:
                        politicians.append({
                            'name': name,
                            'position': 'ì‹œì˜ì›',
                            'district': member.get('district', ''),
                            'party': member.get('party', '')
                        })
    
    return politicians


def load_gu_mayor_data():
    """êµ¬ì²­ì¥ ë°ì´í„° ë¡œë“œ"""
    if not os.path.exists(GU_MAYOR_FILE):
        return []
    
    with open(GU_MAYOR_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    politicians = []
    for gu_name, member_data in data.items():
        if isinstance(member_data, dict):
            raw_name = member_data.get('name', '')
            # í•œì ì œê±°: "ì •ë¬¸í—Œ (é„­æ–‡æ†²)" â†’ "ì •ë¬¸í—Œ" ë˜ëŠ” "ì •ë¬¸í—Œ\n(é„­æ–‡æ†²)" â†’ "ì •ë¬¸í—Œ"
            if '(' in raw_name:
                name = raw_name.split('(')[0].split('\n')[0].strip()
            else:
                name = raw_name.split('\n')[0].strip()
            
            if name:
                politicians.append({
                    'name': name,
                    'position': 'êµ¬ì²­ì¥',
                    'district': gu_name,
                    'party': member_data.get('party', '')
                })
    
    return politicians


def load_gu_uiwon_data():
    """êµ¬ì˜ì› ë°ì´í„° ë¡œë“œ"""
    if not os.path.exists(GU_UIWON_FILE):
        return []
    
    with open(GU_UIWON_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    politicians = []
    for gu_name, members_list in data.items():
        if isinstance(members_list, list):
            for member in members_list:
                if isinstance(member, dict):
                    name = member.get('name', '').split('\n')[0]
                    if name:
                        politicians.append({
                            'name': name,
                            'position': 'êµ¬ì˜ì›',
                            'district': member.get('district', gu_name),
                            'party': member.get('party', '')
                        })
    
    return politicians


def collect_politician_news(politician):
    """ì •ì¹˜ì¸ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    name = politician['name']
    position = politician['position']
    
    articles = []
    
    # ì‹œì¥ì€ êµ­ì •ê°ì‚¬ + í–‰ì •ì‚¬ë¬´ê°ì‚¬ (ê° 50ê±´, 5í˜ì´ì§€)
    if position == 'ì‹œì¥':
        # êµ­ì •ê°ì‚¬
        query1 = f"{name} êµ­ì •ê°ì‚¬"
        print(f"  ğŸ” ê²€ìƒ‰ 1: {query1} (5í˜ì´ì§€)")
        for page in range(1, 6):  # 1~5í˜ì´ì§€
            result1 = search_naver_news(query1, display=10, start=(page-1)*10+1)
            if result1 and 'items' in result1:
                for item in result1['items']:
                    articles.append({
                        'title': item['title'].replace('<b>', '').replace('</b>', ''),
                        'description': item.get('description', '').replace('<b>', '').replace('</b>', ''),
                        'link': item['link'],
                        'pubDate': item['pubDate'],
                        'originallink': item.get('originallink', ''),
                        'search_type': 'êµ­ì •ê°ì‚¬'
                    })
            time.sleep(0.1)
        
        # í–‰ì •ì‚¬ë¬´ê°ì‚¬
        query2 = f"{name} í–‰ì •ì‚¬ë¬´ê°ì‚¬"
        print(f"  ğŸ” ê²€ìƒ‰ 2: {query2} (5í˜ì´ì§€)")
        for page in range(1, 6):  # 1~5í˜ì´ì§€
            result2 = search_naver_news(query2, display=10, start=(page-1)*10+1)
            if result2 and 'items' in result2:
                for item in result2['items']:
                    articles.append({
                        'title': item['title'].replace('<b>', '').replace('</b>', ''),
                        'description': item.get('description', '').replace('<b>', '').replace('</b>', ''),
                        'link': item['link'],
                        'pubDate': item['pubDate'],
                        'originallink': item.get('originallink', ''),
                        'search_type': 'í–‰ì •ì‚¬ë¬´ê°ì‚¬'
                    })
            time.sleep(0.1)
    
    # êµ¬ì²­ì¥ì€ í–‰ì •ì‚¬ë¬´ê°ì‚¬ë§Œ (50ê±´, 5í˜ì´ì§€)
    elif position == 'êµ¬ì²­ì¥':
        query = f"{name} êµ¬ì²­ì¥ í–‰ì •ì‚¬ë¬´ê°ì‚¬"
        print(f"  ğŸ” ê²€ìƒ‰: {query} (5í˜ì´ì§€)")
        for page in range(1, 6):  # 1~5í˜ì´ì§€
            result = search_naver_news(query, display=10, start=(page-1)*10+1)
            if result and 'items' in result:
                for item in result['items']:
                    articles.append({
                        'title': item['title'].replace('<b>', '').replace('</b>', ''),
                        'description': item.get('description', '').replace('<b>', '').replace('</b>', ''),
                        'link': item['link'],
                        'pubDate': item['pubDate'],
                        'originallink': item.get('originallink', ''),
                        'search_type': 'í–‰ì •ì‚¬ë¬´ê°ì‚¬'
                    })
            time.sleep(0.1)
    
    # ì‹œì˜ì›, êµ¬ì˜ì›ì€ í–‰ì •ì‚¬ë¬´ê°ì‚¬ë§Œ (30ê±´, 3í˜ì´ì§€)
    else:
        query = f"{name} {position} í–‰ì •ì‚¬ë¬´ê°ì‚¬"
        print(f"  ğŸ” ê²€ìƒ‰: {query} (3í˜ì´ì§€)")
        for page in range(1, 4):  # 1~3í˜ì´ì§€
            result = search_naver_news(query, display=10, start=(page-1)*10+1)
            if result and 'items' in result:
                for item in result['items']:
                    articles.append({
                        'title': item['title'].replace('<b>', '').replace('</b>', ''),
                        'description': item.get('description', '').replace('<b>', '').replace('</b>', ''),
                        'link': item['link'],
                        'pubDate': item['pubDate'],
                        'originallink': item.get('originallink', ''),
                        'search_type': 'í–‰ì •ì‚¬ë¬´ê°ì‚¬'
                    })
            time.sleep(0.1)
    
    print(f"  âœ… {len(articles)}ê±´ ìˆ˜ì§‘")
    return articles


def load_existing_data():
    """ê¸°ì¡´ ìˆ˜ì§‘ ë°ì´í„° ë¡œë“œ"""
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}


def save_data(data):
    """ë°ì´í„° ì €ì¥"""
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {OUTPUT_FILE}")


def collect_all_news():
    """ëª¨ë“  ì§€ë°© ì •ì¹˜ì¸ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    print("\n" + "="*60)
    print(f"ğŸ• ì§€ë°© ì •ì¹˜ì¸ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60 + "\n")
    
    # ì¸í„°ë„· ì—°ê²° í™•ì¸
    if not check_internet_connection():
        print("âŒ ì¸í„°ë„· ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ì˜¤í”„ë¼ì¸ ëª¨ë“œ: ê¸°ì¡´ ë°ì´í„°ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.")
        
        if os.path.exists(OUTPUT_FILE):
            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
            print(f"âœ… ê¸°ì¡´ ë°ì´í„° ìœ ì§€: {len(existing_data)}ëª…")
        
        return
    
    print("âœ… ì¸í„°ë„· ì—°ê²° í™•ì¸ ì™„ë£Œ\n")
    
    # ì •ì¹˜ì¸ ë°ì´í„° ë¡œë“œ
    print("ğŸ“‚ ì •ì¹˜ì¸ ë°ì´í„° ë¡œë”© ì¤‘...")
    mayors = load_mayor_data()
    si_uiwons = load_si_uiwon_data()
    gu_mayors = load_gu_mayor_data()
    gu_uiwons = load_gu_uiwon_data()
    
    all_politicians = mayors + si_uiwons + gu_mayors + gu_uiwons
    
    print(f"âœ… ë¡œë“œ ì™„ë£Œ:")
    print(f"  - ì‹œì¥: {len(mayors)}ëª…")
    print(f"  - ì‹œì˜ì›: {len(si_uiwons)}ëª…")
    print(f"  - êµ¬ì²­ì¥: {len(gu_mayors)}ëª…")
    print(f"  - êµ¬ì˜ì›: {len(gu_uiwons)}ëª…")
    print(f"  - ì´ê³„: {len(all_politicians)}ëª…\n")
    
    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    all_data = load_existing_data()
    
    # ê° ì •ì¹˜ì¸ë³„ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘
    collected_count = 0
    failed_count = 0
    
    for i, politician in enumerate(all_politicians, 1):
        name = politician['name']
        position = politician['position']
        
        print(f"[{i}/{len(all_politicians)}] {name} ({position}, {politician['district']})")
        
        # ë‰´ìŠ¤ ìˆ˜ì§‘
        articles = collect_politician_news(politician)
        
        if articles:
            # ê¸°ì¡´ ë°ì´í„°ì— ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
            if name not in all_data:
                all_data[name] = {
                    'politician_info': politician,
                    'collected_date': datetime.now().strftime('%Y%m%d'),
                    'total_count': 0,
                    'news': []
                }
            
            # ì¤‘ë³µ ì²´í¬ (ë§í¬ ê¸°ì¤€)
            existing_links = {article['link'] for article in all_data[name]['news']}
            new_articles = [a for a in articles if a['link'] not in existing_links]
            
            all_data[name]['news'].extend(new_articles)
            all_data[name]['total_count'] = len(all_data[name]['news'])
            all_data[name]['last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            collected_count += len(new_articles)
            print(f"  ğŸ“° ì‹ ê·œ ê¸°ì‚¬: {len(new_articles)}ê±´ (ì „ì²´: {all_data[name]['total_count']}ê±´)")
        else:
            failed_count += 1
            print(f"  âŒ ìˆ˜ì§‘ ì‹¤íŒ¨")
        
        # API í˜¸ì¶œ ì œí•œ - 0.1ì´ˆ ëŒ€ê¸°
        time.sleep(0.1)
        
        # 50ëª…ë§ˆë‹¤ ì¤‘ê°„ ì €ì¥
        if i % 50 == 0:
            save_data(all_data)
            print(f"\nğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ ({i}/{len(all_politicians)})\n")
    
    # ìµœì¢… ì €ì¥
    save_data(all_data)
    
    print("\n" + "="*60)
    print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: ì´ {collected_count}ê±´ì˜ ì‹ ê·œ ê¸°ì‚¬")
    if failed_count > 0:
        print(f"âš ï¸  ìˆ˜ì§‘ ì‹¤íŒ¨: {failed_count}ëª…")
    print(f"ğŸ“Š ì „ì²´ ë°ì´í„°: {len(all_data)}ëª…ì˜ ì •ì¹˜ì¸, {sum(d['total_count'] for d in all_data.values())}ê±´ì˜ ê¸°ì‚¬")
    print(f"ğŸ• ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60 + "\n")


def schedule_daily_collection():
    """ë§¤ì¼ 3íšŒ ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ ì„¤ì •"""
    schedule.every().day.at("09:00").do(collect_all_news)
    schedule.every().day.at("15:00").do(collect_all_news)
    schedule.every().day.at("21:00").do(collect_all_news)
    
    print("â° ì§€ë°© ì •ì¹˜ì¸ ë‰´ìŠ¤ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘")
    print("ğŸ“… ìˆ˜ì§‘ ì‹œê°„: ë§¤ì¼ 09:00, 15:00, 21:00")
    print("ğŸ’¡ ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”\n")
    
    # ì²« ì‹¤í–‰
    collect_all_news()
    
    # ìŠ¤ì¼€ì¤„ ì‹¤í–‰
    while True:
        schedule.run_pending()
        time.sleep(60)


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--once":
        collect_all_news()
    elif len(sys.argv) > 1 and sys.argv[1] == "--schedule":
        try:
            schedule_daily_collection()
        except KeyboardInterrupt:
            print("\n\nâ¹ï¸  ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ")
    else:
        print("ì‚¬ìš©ë²•:")
        print("  python3 collect_local_politicians_news.py --once      # 1íšŒ ì‹¤í–‰")
        print("  python3 collect_local_politicians_news.py --schedule  # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰ (ë§¤ì¼ 3íšŒ)")

